import time
import random
import requests
import pandas as pd
from datetime import datetime
import os
import threading
from flask import Flask
import logging
import urllib3

# é—œé–‰ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)

app = Flask(__name__)

# --- é…ç½®å€ ---
TOKEN = os.environ.get("TG_TOKEN", "ä½ çš„TOKEN")
# ä½ çš„åŸå§‹ IDï¼Œä½œç‚ºä¿åº•ï¼Œé¿å… Render é‡å•Ÿå¾Œæ¸…å–®æ¶ˆå¤±
ADMIN_CHAT_ID = os.environ.get("TG_CHAT_ID", "ä½ çš„ID")
HISTORY_FILE = "/tmp/sent_jobs_history.csv" 
USERS_FILE = "/tmp/subscribers.csv" # è¨˜éŒ„æ‰€æœ‰é»æ“Š start çš„ä½¿ç”¨è€…

# --- Web ä¼ºæœå™¨è¨­å®š (é˜²æ­¢ Render ä¼‘çœ ) ---
@app.route('/')
def health_check():
    return f"Bot is running. Active users: {len(get_all_users())}. Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", 200

# --- ä½¿ç”¨è€…ç®¡ç†é‚è¼¯ ---
def get_all_users():
    """å–å¾—æ‰€æœ‰è¨‚é–±è€… ID"""
    users = {ADMIN_CHAT_ID} # ä½¿ç”¨ set é¿å…é‡è¤‡ï¼Œä¸¦åŠ å…¥ä¿åº• ID
    if os.path.exists(USERS_FILE):
        try:
            df = pd.read_csv(USERS_FILE)
            file_users = df['chat_id'].astype(str).tolist()
            users.update(file_users)
        except:
            pass
    return list(users)

def save_new_user(chat_id):
    """å­˜å…¥æ–°ä½¿ç”¨è€…"""
    chat_id = str(chat_id)
    users = get_all_users()
    if chat_id not in users:
        df = pd.DataFrame({'chat_id': [chat_id]})
        header = not os.path.exists(USERS_FILE)
        df.to_csv(USERS_FILE, mode='a', index=False, header=header)
        logging.info(f"æ–°ä½¿ç”¨è€…è¨‚é–±: {chat_id}")
        return True
    return False

def check_for_updates():
    """è¼ªè©¢ Telegram API æª¢æŸ¥æ˜¯å¦æœ‰æ–°ä½¿ç”¨è€…å‚³é€ /start"""
    last_update_id = 0
    logging.info("ä½¿ç”¨è€…ç›£æ¸¬åŸ·è¡Œç·’å•Ÿå‹•...")
    while True:
        try:
            url = f"https://api.telegram.org/bot{TOKEN}/getUpdates?offset={last_update_id + 1}&timeout=30"
            r = requests.get(url, timeout=35).json()
            if r.get("result"):
                for update in r["result"]:
                    last_update_id = update["update_id"]
                    if "message" in update and "text" in update["message"]:
                        msg = update["message"]
                        chat_id = str(msg["chat"]["id"])
                        text = msg["text"]
                        
                        if text == "/start":
                            if save_new_user(chat_id):
                                welcome_text = "ğŸ‰ æ­¡è¿ä½¿ç”¨ 104 è·ç¼ºè¿½è¹¤æ©Ÿå™¨äººï¼ç•¶æœ‰ç¬¦åˆ Python çš„æ–°è·ç¼ºæ™‚ï¼Œæˆ‘æœƒç«‹å³é€šçŸ¥æ‚¨ã€‚"
                                requests.post(f"https://api.telegram.org/bot{TOKEN}/sendMessage", 
                                              data={"chat_id": chat_id, "text": welcome_text})
        except Exception as e:
            logging.error(f"ç›£æ¸¬æ›´æ–°å¤±æ•—: {e}")
        time.sleep(5)

# --- è·ç¼ºç™¼é€é‚è¼¯ ---
def send_tg_broadcast(text):
    """å°‡è¨Šæ¯ç™¼é€çµ¦æ‰€æœ‰è¨‚é–±è€…"""
    users = get_all_users()
    for user_id in users:
        url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
        payload = {"chat_id": user_id, "text": text, "parse_mode": "HTML"}
        try:
            requests.post(url, data=payload)
        except Exception as e:
            logging.error(f"ç™¼é€çµ¦ {user_id} å¤±æ•—: {e}")
    time.sleep(1) # å»£æ’­å®Œç•¢å°ä¼‘

# --- 104 çˆ¬èŸ²é¡åˆ¥ ---
class Job104Spider():
    def search(self, keyword, max_mun=10, filter_params=None, sort_type='ç¬¦åˆåº¦', is_sort_asc=False):
        jobs = []
        total_count = 0
        url = 'https://www.104.com.tw/jobs/search/list'
        query = f'ro=0&kwop=7&keyword={keyword}&expansionType=area,spec,com,job,wf,wktm&mode=s&jobsource=2018indexpoc'
        if filter_params:
            query += ''.join([f'&{key}={value}' for key, value, in filter_params.items()])
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.92 Safari/537.36',
            'Referer': 'https://www.104.com.tw/jobs/search/',
        }
        sort_dict = {'ç¬¦åˆåº¦': '1', 'æ—¥æœŸ': '2', 'ç¶“æ­·': '3', 'å­¸æ­·': '4', 'æ‡‰å¾µäººæ•¸': '7', 'å¾…é‡': '13'}
        sort_params = f"&order={sort_dict.get(sort_type, '1')}" + ('&asc=1' if is_sort_asc else '&asc=0')
        query += sort_params

        page = 1
        while len(jobs) < max_mun:
            params = f'{query}&page={page}'
            r = requests.get(url, params=params, headers=headers)
            if r.status_code != requests.codes.ok:
                break
            data = r.json()
            total_count = data['data']['totalCount']
            jobs.extend(data['data']['list'])
            if (page == data['data']['totalPage']) or (data['data']['totalPage'] == 0):
                break
            page += 1
            time.sleep(random.uniform(3, 5))
        return total_count, jobs[:max_mun]

    def search_job_transform(self, job_data):
        job_url = f"https:{job_data['link']['job']}"
        job_id = job_url.split('/job/')[-1].split('?')[0]
        return {
            'job_id': job_id,
            'name': job_data['jobName'],
            'appear_date': job_data['appearDate'],
            'company_name': job_data['custName'],
            'company_addr': f"{job_data['jobAddrNoDesc']} {job_data['jobAddress']}",
            'job_url': job_url,
            'education': job_data['optionEdu'],
            'period': job_data['periodDesc'],
            'salary': job_data['salaryDesc'],
        }

# --- çˆ¬èŸ²ä¸»è¿´åœˆ ---
def run_spider_loop():
    logging.info("çˆ¬èŸ²åŸ·è¡Œç·’å•Ÿå‹•...")
    while True:
        try:
            logging.info("é–‹å§‹æƒæ 104 è·ç¼º...")
            spider = Job104Spider()
            filter_params = {'newZone': '1'}
            _, raw_jobs = spider.search('python', max_mun=50, filter_params=filter_params)
            jobs = [spider.search_job_transform(j) for j in raw_jobs]
            
            sent_job_ids = set()
            if os.path.exists(HISTORY_FILE):
                df_h = pd.read_csv(HISTORY_FILE)
                sent_job_ids = set(df_h['job_id'].astype(str).tolist())

            df_all = pd.DataFrame(jobs)
            today_str = datetime.now().strftime('%Y%m%d')
            new_jobs = df_all[(df_all['appear_date'] == today_str) & (~df_all['job_id'].astype(str).isin(sent_job_ids))]

            if not new_jobs.empty:
                for _, row in new_jobs.iterrows():
                    msg = f"ğŸ”¹ <b>{row['name']}</b>\nğŸ¢ å…¬å¸ï¼š{row['company_name']}\nğŸ’° å¾…é‡ï¼š{row['salary']}\nğŸ“ å­¸æ­·ï¼š{row['education']} / {row['period']}\nğŸ“ åœ°é»ï¼š{row['company_addr']}\nğŸ”— <a href='{row['job_url']}'>æŸ¥çœ‹è©³æƒ…</a>"
                    send_tg_broadcast(msg)
                    time.sleep(1)

                new_ids_df = pd.DataFrame({'job_id': new_jobs['job_id'].astype(str)})
                new_ids_df.to_csv(HISTORY_FILE, mode='a', header=not os.path.exists(HISTORY_FILE), index=False)
            
            logging.info(f"æƒæçµæŸã€‚æ–°å¢: {len(new_jobs)} ç­†ã€‚")
        except Exception as e:
            logging.error(f"çˆ¬èŸ²è¿´åœˆå‡ºéŒ¯: {e}")
        
        time.sleep(1200) # 20åˆ†é˜æŠ“ä¸€æ¬¡

if __name__ == "__main__":
    # å•Ÿå‹•çˆ¬èŸ²åŸ·è¡Œç·’
    threading.Thread(target=run_spider_loop, daemon=True).start()
    # å•Ÿå‹•ä½¿ç”¨è€…ç›£æ¸¬åŸ·è¡Œç·’
    threading.Thread(target=check_for_updates, daemon=True).start()
    
    # å•Ÿå‹• Web æœå‹™
    port = int(os.environ.get("PORT", 10000))
    app.run(host='0.0.0.0', port=port)
